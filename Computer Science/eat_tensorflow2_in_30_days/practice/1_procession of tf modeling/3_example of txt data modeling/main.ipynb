{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600088073524",
   "display_name": "Python 3.7.9 64-bit ('TrajDetec': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 一，准备数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,string\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../../../data/imdb/train.csv\"\n",
    "test_data_path = \"../../../data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    arr = tf.strings.split(line, \"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\n",
    "    # tf.cast 用于数据类型转换\n",
    "    # tf.expand_dims 用于增加维度\n",
    "    text = tf.expand_dims(arr[1], axis=0)\n",
    "    return text,label\n",
    "\n",
    "ds_train_raw = tf.data.TextLineDataset(filenames=train_data_path) \\\n",
    "    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "        .shuffle(buffer_size=1000) \\\n",
    "            .batch(BATCH_SIZE) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames=test_data_path) \\\n",
    "    .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not']\n"
    }
   ],
   "source": [
    "def clearn_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(\n",
    "        stripped_html,\n",
    "        '[%s]' % re.escape(string.punctuation),\n",
    "        ''\n",
    "        )\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clearn_text,\n",
    "    split='whitespace',\n",
    "    max_tokens=MAX_WORDS-1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "source": [
    "## 二，定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\n",
    "        self.pool_1 = layers.MaxPool1D(name = \"pool_1\")\n",
    "        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\n",
    "        self.pool_2 = layers.MaxPool1D(name = \"pool_2\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(CnnModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n",
    "    \n",
    "    # 用于显示Output Shape\n",
    "    def summary(self):\n",
    "        x_input = layers.Input(shape = MAX_LEN)\n",
    "        output = self.call(x_input)\n",
    "        model = tf.keras.Model(inputs = x_input,outputs = output)\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 200)]             0         \n_________________________________________________________________\nembedding (Embedding)        (None, 200, 7)            70000     \n_________________________________________________________________\nconv_1 (Conv1D)              (None, 196, 16)           576       \n_________________________________________________________________\npool_1 (MaxPooling1D)        (None, 98, 16)            0         \n_________________________________________________________________\nconv_2 (Conv1D)              (None, 97, 128)           4224      \n_________________________________________________________________\npool_2 (MaxPooling1D)        (None, 48, 128)           0         \n_________________________________________________________________\nflatten (Flatten)            (None, 6144)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 6145      \n=================================================================\nTotal params: 80,945\nTrainable params: 80,945\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = CnnModel()\n",
    "model.build(input_shape =(None,MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## 三，训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    today_ts = tf.timestamp()%(24*60*60)\n",
    "    \n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8+timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "================================================================================10:17:39\nEpoch=1,Loss:0.451597869,Accuracy:0.758,Valid Loss:0.382119209,Valid Accuracy:0.8308\n\n================================================================================10:17:43\nEpoch=2,Loss:0.243866518,Accuracy:0.904,Valid Loss:0.322016329,Valid Accuracy:0.866\n\n================================================================================10:17:47\nEpoch=3,Loss:0.167434573,Accuracy:0.93755,Valid Loss:0.357636,Valid Accuracy:0.868\n\n================================================================================10:17:51\nEpoch=4,Loss:0.105727315,Accuracy:0.964,Valid Loss:0.473848879,Valid Accuracy:0.8572\n\n================================================================================10:17:55\nEpoch=5,Loss:0.0603662916,Accuracy:0.98125,Valid Loss:0.599012673,Valid Accuracy:0.8568\n\n================================================================================10:17:58\nEpoch=6,Loss:0.0302702207,Accuracy:0.99075,Valid Loss:0.805248082,Valid Accuracy:0.8578\n\n"
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features,training = False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "        \n",
    "        #此处logs模板需要根据metric具体情况修改\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' \n",
    "        \n",
    "        if epoch%1==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,epochs = 6)"
   ]
  },
  {
   "source": [
    "## 四，评估模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "         valid_step(model,features,labels)\n",
    "    logs = 'Valid Loss:{},Valid Accuracy:{}' \n",
    "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Valid Loss:0.805248082,Valid Accuracy:0.8578\n"
    }
   ],
   "source": [
    "evaluate_model(model,ds_test)"
   ]
  },
  {
   "source": [
    "## 五，使用模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.99278486],\n       [0.9999759 ],\n       [0.99926895],\n       ...,\n       [0.10119107],\n       [0.19066869],\n       [1.        ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[9.92784858e-01]\n [9.99975920e-01]\n [9.99268949e-01]\n [5.72908993e-06]\n [7.66376317e-01]\n [1.68699025e-05]\n [2.09160822e-09]\n [1.17711592e-04]\n [9.99951839e-01]\n [9.94080007e-01]\n [9.99999642e-01]\n [9.99793112e-01]\n [5.23628478e-06]\n [6.86631203e-01]\n [1.19694805e-08]\n [9.73942876e-01]\n [1.27842053e-04]\n [5.36034822e-01]\n [5.98479701e-08]\n [9.33194220e-01]], shape=(20, 1), dtype=float32)\n"
    }
   ],
   "source": [
    "for x_test,_ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    #以下方法等价：\n",
    "    #print(model.call(x_test))\n",
    "    #print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "source": [
    "## 六，保存模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: ./data/tf_model_savedmodel\\assets\nexport saved model.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.99278486],\n       [0.9999759 ],\n       [0.99926895],\n       ...,\n       [0.10119107],\n       [0.19066869],\n       [1.        ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}