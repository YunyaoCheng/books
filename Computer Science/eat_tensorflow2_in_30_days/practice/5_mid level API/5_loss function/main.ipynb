{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600739526666",
   "display_name": "Python 3.7.9 64-bit ('TrajDetec': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "source": [
    "## 一，损失函数和正则化项"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 64)                4160      \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                650       \n=================================================================\nTotal params: 4,810\nTrainable params: 4,810\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(\n",
    "    layers.Dense(\n",
    "        64,\n",
    "        input_dim=64,\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        activity_regularizer=regularizers.l1(0.01),\n",
    "        kernel_constraint=constraints.MaxNorm(max_value=2, axis=0)\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    layers.Dense(\n",
    "        10,\n",
    "        kernel_regularizer=regularizers.l1_l2(0.01, 0.01),\n",
    "        activation='sigmoid'\n",
    "    )\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['AUC']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## 二，内置损失函数\n",
    "\n",
    "\n",
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "常用的一些内置损失函数说明如下。\n",
    "\n",
    "* mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）\n",
    "\n",
    "* mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)\n",
    "\n",
    "* Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)\n",
    "\n",
    "* binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)\n",
    "\n",
    "* hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)\n",
    "\n",
    "* kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)\n",
    "\n",
    "* cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 三，自定义损失函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2, alpha=0.75):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = (y_true*y_pred) + ((1-y_true)*(i-y_pred))\n",
    "        alpha_factor = y_true * alpha + (1-y_true) * (1-alpha)\n",
    "        modulating_factor = tf.pow(1.0-p_t, gamma)\n",
    "        loss = tf.reduce_sum(alpha_factor*modulating_factor*bce, axis=-1)\n",
    "        return loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.75,name = \"focal_loss\"):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, self.gamma)\n",
    "        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = -1 )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}